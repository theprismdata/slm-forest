# Phi-2 Fine-tuning Configuration for Tax Law Q&A

# Model Configuration
model_name: "microsoft/phi-2"
model_type: "phi"
max_length: 2048
padding: "max_length"
truncation: true

# Training Configuration
training_args:
  output_dir: "/workspace/phi2-tax-law-finetuned"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  fp16: true
  dataloader_pin_memory: false
  remove_unused_columns: false
  push_to_hub: false
  report_to: "none"
  run_name: "phi2-tax-law-qa"

# LoRA Configuration
lora_config:
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "out_proj", "fc1", "fc2"]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

# Dataset Configuration
dataset:
  train_file: "fine-tunning-ds/distillation_legal_qa_dataset.json"
  validation_split: 0.1
  max_train_samples: null  # Use all samples
  max_eval_samples: null   # Use all samples

# Generation Configuration
generation_config:
  max_new_tokens: 512
  do_sample: true
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1
  pad_token_id: 50256
  eos_token_id: 50256

# Prompt Template
prompt_template: |
  ### 질문: {question}

  ### 답변: {answer} 