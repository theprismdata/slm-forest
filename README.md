# SLM Forest - μ„Έλ²• Q&A νμΈνλ‹ ν”„λ΅μ νΈ
μ΄ ν”„λ΅μ νΈλ” Small Language Models(SLM)μ„ μ„Έλ²• Q&A λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•μ—¬ μ‹¤μ©μ μΈ μ„Έλ²• μ§μμ‘λ‹µ μ‹μ¤ν…μ„ κµ¬μ¶•ν•λ” ν”„λ΅μ νΈμ…λ‹λ‹¤.

## π― ν”„λ΅μ νΈ κ°μ”

- **ν„μ¬ λ¨λΈ**: Microsoft Phi-2 (2.7B νλΌλ―Έν„°) - ν•κΈ€ μ„±λ¥ μ ν•μΌλ΅ μΈν• μ°¨μ„Έλ€ λ¨λΈ κ²€ν†  μ¤‘
- **λ‹¤μ λ‹¨κ³„**: Google Gemma3:12b νμΈνλ‹ μμ • (ν–¥μƒλ ν•κΈ€ μ§€μ›)
- **λ°μ΄ν„°μ…‹**: μ‹¤μ©μ μΈ μ„Έλ²• Q&A λ°μ΄ν„° (OpenAPI + νλ΅€ λ°μ΄ν„° κΈ°λ°)
- **ν›λ ¨ ν™κ²½**: RunPod (GPU ν΄λΌμ°λ“)
- **μ¶”λ΅  ν™κ²½**: λ΅μ»¬ ν™κ²½ (M4 MacBook Pro μ„±λ¥ μ ν• ν™•μΈλ¨)

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
slm-forest/
β”β”€β”€ π“„ README.md                          # ν”„λ΅μ νΈ λ©”μΈ λ¬Έμ„
β”β”€β”€ π“„ .gitignore                         # Git μ μ™Έ νμΌ μ„¤μ •
β”‚
β”β”€β”€ π“ slm_datagen/                       # λ°μ΄ν„° μƒμ„± λ¨λ“
β”‚   β”β”€β”€ π“„ gen_finetunningds.py          # νμΈνλ‹ λ°μ΄ν„°μ…‹ μƒμ„± μ¤ν¬λ¦½νΈ (20KB)
β”‚   β”β”€β”€ π“„ setup_m4_mac.sh               # M4 Mac ν™κ²½ μ„¤μ • μ¤ν¬λ¦½νΈ
β”‚   β””β”€β”€ π“ tax-law-gen-raw-data/         # μ›μ‹ λ°μ΄ν„° (GitHubμ—μ„ μ μ™Έλ¨)
β”‚       β”β”€β”€ π“„ config.yaml               # API μ„¤μ • νμΌ
β”‚       β”β”€β”€ π“ tax-law-openapi/          # μ„Έλ²• OpenAPI λ°μ΄ν„° (12κ° λ¶„μ•Ό)
β”‚       β”β”€β”€ π“ tax-law-judgment/         # μ„Έλ²• νλ΅€ λ°μ΄ν„° (μ΄ 417MB)
β”‚       β””β”€β”€ π“ fine-tunning-ds/          # νμΈνλ‹ λ°μ΄ν„°μ…‹
β”‚           β””β”€β”€ π“„ distillation_legal_qa_dataset.json # μµμΆ… Q&A λ°μ΄ν„°μ…‹ (5MB)
β”‚
β”β”€β”€ π“ fine-tunning/                     # λ¨λΈ ν›λ ¨ λ¨λ“
β”‚   β””β”€β”€ π“ phi2/                         # Phi-2 ν›λ ¨ μ„¤μ •
β”‚       β”β”€β”€ π“„ finetuning-phi2.py       # Phi-2 νμΈνλ‹ μ¤ν¬λ¦½νΈ (19KB)
β”‚       β”β”€β”€ π“„ train_config.yaml        # ν›λ ¨ μ„¤μ • νμΌ
β”‚       β”β”€β”€ π“„ runpod_train.sh          # RunPod ν›λ ¨ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚       β””β”€β”€ π“„ requirements-finetunning.txt # ν›λ ¨ ν™κ²½ μμ΅΄μ„±
β”‚
β””β”€β”€ π“ model_serving/                    # λ¨λΈ μ„λΉ™ λ¨λ“ (GitHubμ—μ„ μ μ™Έλ¨)
    β”β”€β”€ π“„ phi-2-inference.py           # Phi-2 μ¶”λ΅  μ¤ν¬λ¦½νΈ
    β”β”€β”€ π“„ phi-2-fintunning-inference.py # νμΈνλ‹λ Phi-2 μ¶”λ΅  μ¤ν¬λ¦½νΈ (12KB)
    β”β”€β”€ π“„ mistral-inference.py         # Mistral μ¶”λ΅  μ¤ν¬λ¦½νΈ
    β””β”€β”€ π“„ requirements-inference.txt    # μ¶”λ΅  ν™κ²½ μμ΅΄μ„±
```

## π€ λΉ λ¥Έ μ‹μ‘

### 1. λ°μ΄ν„°μ…‹ μƒμ„±

#### ν™κ²½ μ„¤μ •
```bash
cd slm_datagen
pip install -r requirements.txt
```

#### API μ„¤μ •
`tax-law-gen-raw-data/config.yaml` νμΌμ—μ„ API ν‚¤ μ„¤μ •:
```yaml
langmodel:
  API:
    OpenAI:
      apikey: "your-openai-api-key"
      chat_model: "gpt-4o"
    Claude:
      apikey: "your-claude-api-key"
      chat_model: "claude-3-sonnet-20240229"
```

#### λ°μ΄ν„°μ…‹ μƒμ„± μ‹¤ν–‰
```bash
python gen_finetunningds.py
```

### 2. RunPodμ—μ„ ν›λ ¨ν•κΈ°

#### RunPod μ„¤μ •
1. RunPodμ—μ„ GPU μΈμ¤ν„΄μ¤ μƒμ„± (RTX 4090, A100 λ“± κ¶μ¥)
2. ν”„λ΅μ νΈ νμΌλ“¤μ„ RunPodμ— μ—…λ΅λ“
3. ν„°λ―Έλ„μ—μ„ λ‹¤μ λ…λ Ήμ–΄ μ‹¤ν–‰:

```bash
cd fine-tunning/phi2

# μ‹¤ν–‰ κ¶ν• λ¶€μ—¬
chmod +x runpod_train.sh

# ν›λ ¨ μ‹μ‘
./runpod_train.sh
```

#### ν›λ ¨ μ„¤μ • μ΅°μ •
`train_config.yaml` νμΌμ—μ„ λ‹¤μ μ„¤μ •μ„ μ΅°μ •ν•  μ μμµλ‹λ‹¤:

```yaml
training_args:
  num_train_epochs: 3                    # ν›λ ¨ μ—ν¬ν¬
  per_device_train_batch_size: 4         # λ°°μΉ ν¬κΈ°
  learning_rate: 2e-5                    # ν•™μµλ¥ 
  gradient_accumulation_steps: 4         # κ·Έλλ””μ–ΈνΈ λ„μ 

lora_config:
  r: 16                                  # LoRA λ­ν¬
  lora_alpha: 32                         # LoRA μ•ν
  lora_dropout: 0.1                      # LoRA λ“λ΅­μ•„μ›ƒ
```

### 3. λ¨λΈ μ¶”λ΅  (ν„μ¬ μ ν•μ‚¬ν•­ μμ)

#### ν™κ²½ μ„¤μ •
```bash
cd model_serving
pip install -r requirements-inference.txt
```

#### μ¶”λ΅  μ‹¤ν–‰
```bash
# κΈ°λ³Έ Phi-2 μ¶”λ΅ 
python phi-2-inference.py --question "λ¶€κ°€κ°€μΉμ„Έ μ‹ κ³ λ” μ–΄λ–»κ² ν•λ‚μ”?"

# νμΈνλ‹λ λ¨λΈ μ¶”λ΅ 
python phi-2-fintunning-inference.py --interactive
```

## π“ λ°μ΄ν„°μ…‹ μ •λ³΄

### μ›μ‹ λ°μ΄ν„° μ†μ¤
- **OpenAPI λ°μ΄ν„°**: 12κ° μ„Έλ²• λ¶„μ•Όλ³„ κµ¬μ΅°ν™”λ λ°μ΄ν„°
- **νλ΅€ λ°μ΄ν„°**: μ΄ 417MBμ μ„Έλ²• κ΄€λ ¨ νλ΅€ λ° ν•΄μ„ λ°μ΄ν„°
- **μµμΆ… λ°μ΄ν„°μ…‹**: 5MBμ Q&A ν•νƒλ΅ μ •μ λ λ°μ΄ν„°

### λ°μ΄ν„° μƒμ„± νμ΄ν”„λΌμΈ
1. **μ›μ‹ λ°μ΄ν„° μμ§‘**: OpenAPI λ° νλ΅€ λ°μ΄ν„° ν¬λ΅¤λ§
2. **λ°μ΄ν„° μ •μ **: μ¤‘λ³µ μ κ±°, ν•μ‹ ν†µμΌ
3. **Q&A λ³€ν™**: LLMμ„ ν™μ©ν• μ§λ¬Έ-λ‹µλ³€ μ μƒμ„±
4. **ν’μ§ κ²€μ¦**: μλ™ν™”λ ν’μ§ μ²΄ν¬ λ° μλ™ κ²€ν† 

### λ°μ΄ν„°μ…‹ μμ‹
```json
{
  "question": "λ¶€λ¨λ‹κ» 20μ–µμ› μ¦μ—¬λ°›μ•λ”λ° μ¦μ—¬μ„Έλ” μ–Όλ§λ‚ λλ‚μ”?",
  "answer": "20μ–µμ› μ¦μ—¬μ— λ€ν• μ¦μ—¬μ„Έλ” λ‹¤μκ³Ό κ°™μ΄ κ³„μ‚°λ©λ‹λ‹¤:\n\nγ€μ¦μ—¬μ„Έ κ³„μ‚°γ€‘\n- μ¦μ—¬μ¬μ‚°κ°€μ•΅: 20μ–µμ›\n- κ³µμ : μ—†μ (λ¶€λ¨λ‹ μ¦μ—¬λ” κ³µμ  λ€μƒ μ•„λ‹)\n- κ³Όμ„Έκ°€μ•΅: 20μ–µμ›\n\nγ€μ„Έμ¨ μ μ©γ€‘\n- 1μ–µμ› μ΄ν•: 10%\n- 1μ–µμ› μ΄κ³Ό 5μ–µμ› μ΄ν•: 20%\n- 5μ–µμ› μ΄κ³Ό 10μ–µμ› μ΄ν•: 30%\n\nγ€μ„Έμ•΅ κ³„μ‚°γ€‘\n- 1μ–µμ› Γ— 10% = 1,000λ§μ›\n- 4μ–µμ› Γ— 20% = 8,000λ§μ›\n- μ΄ μ¦μ—¬μ„Έ: 9,000λ§μ›",
  "type": "student",
  "source": "practical"
}
```

## π”§ κΈ°μ μ  μ„Έλ¶€μ‚¬ν•­

### ν„μ¬ λ¨λΈ (Phi-2)
- **λ² μ΄μ¤ λ¨λΈ**: Microsoft Phi-2 (2.7B νλΌλ―Έν„°)
- **νμΈνλ‹ λ°©λ²•**: LoRA (Low-Rank Adaptation)
- **μµμ ν™”**: FP16 ν›λ ¨, κ·Έλλ””μ–ΈνΈ λ„μ 
- **β οΈ ν•κ³„μ **: 
  - ν•κΈ€ μ²λ¦¬ μ„±λ¥ μ ν•
  - M4 MacBook Proμ—μ„ λλ¦° μ¶”λ΅  μ†λ„ (μ΄λ‹Ή 5-10 ν† ν°)
  - λ³µμ΅ν• μ„Έλ²• μ§μμ— λ€ν• μ΄ν•΄λ„ λ¶€μ΅±

### μ°¨μ„Έλ€ λ¨λΈ (Gemma3:12b) - κ³„ν μ¤‘
- **λ² μ΄μ¤ λ¨λΈ**: Google Gemma3:12b (12B νλΌλ―Έν„°)
- **μμƒ μ¥μ **:
  - ν–¥μƒλ ν•κΈ€ μ§€μ›
  - λ” λ‚μ€ μ¶”λ΅  λ¥λ ¥
  - λ³µμ΅ν• λ²•λ¥  λ…Όλ¦¬ μ²λ¦¬ κ°μ„ 
- **ν›λ ¨ κ³„ν**: λ™μΌν• λ°μ΄ν„°μ…‹μΌλ΅ LoRA νμΈνλ‹
- **μΈν”„λΌ**: RunPod A100 GPU ν™μ©

### ν›λ ¨ μ„¤μ •
- **μµλ€ μ‹ν€€μ¤ κΈΈμ΄**: 2,048 ν† ν°
- **ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ**: 
  ```
  ### μ§λ¬Έ: {question}
  
  ### λ‹µλ³€: {answer}
  ```
- **κ²€μ¦ λ¶„ν• **: 10%
- **μ΅°κΈ° μΆ…λ£**: 3 μ—ν¬ν¬ μ—°μ† κ°μ„  μ—†μ„ μ‹

## π’° λΉ„μ© μ¶”μ •

### λ°μ΄ν„° μƒμ„± λΉ„μ©
- **OpenAI API**: μ•½ $50-100 (GPT-4o μ‚¬μ©)
- **Claude API**: μ•½ $30-50 (Claude-3-Sonnet μ‚¬μ©)

### RunPod ν›λ ¨ λΉ„μ©
- **Phi-2 (RTX 4090)**: μ•½ $1.2 - $4.8
- **Gemma3:12b (A100 40GB)**: μ•½ $10 - $20 (μμƒ)

### μ¶”λ΅  λΉ„μ©
- **λ΅μ»¬ μ‹¤ν–‰**: λ¬΄λ£ (μ„±λ¥ μ ν• μμ)
- **ν΄λΌμ°λ“ μ¶”λ΅ **: κ²€ν†  μ¤‘

## π› οΈ λ¬Έμ  ν•΄κ²°

### ν„μ¬ μ•λ ¤μ§„ λ¬Έμ λ“¤

**1. Phi-2 ν•κΈ€ μ„±λ¥ λ¬Έμ **
- λ³µμ΅ν• ν•κΈ€ λ¬Έμ¥ μ΄ν•΄ λ¶€μ΅±
- λ²•λ¥  μ©μ–΄ μ²λ¦¬ μ •ν™•λ„ λ‚®μ
- **ν•΄κ²° λ°©μ•**: Gemma3:12bλ΅ λ¨λΈ μ—…κ·Έλ μ΄λ“

**2. M4 MacBook Pro μ„±λ¥ λ¬Έμ **
- μ¶”λ΅  μ†λ„ λλ¦Ό (μ΄λ‹Ή 5-10 ν† ν°)
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ†’μ (12-16GB)
- **ν•΄κ²° λ°©μ•**: ν΄λΌμ°λ“ μ¶”λ΅  ν™κ²½ κ²€ν† 

**3. μΌλ°μ μΈ κΈ°μ μ  λ¬Έμ **
```bash
# CUDA λ©”λ¨λ¦¬ λ¶€μ΅±
per_device_train_batch_size: 2
gradient_accumulation_steps: 8

# MPS κ΄€λ ¨ μ¤λ¥
python inference.py --device cpu

# λ¨λΈ λ΅λ”© μ‹¤ν¨
rm -rf ~/.cache/huggingface/
```

## π“ μ„±λ¥ λ¨λ‹ν„°λ§

### ν„μ¬ μ„±λ¥ (Phi-2)
- **μ‘λ‹µ μ‹κ°„**: ν‰κ·  10-20μ΄ (M4 Mac)
- **ν† ν° μƒμ„± μ†λ„**: μ΄λ‹Ή 5-10 ν† ν°
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: 12-16GB
- **ν•κΈ€ μ •ν™•λ„**: 60-70% (μ£Όκ΄€μ  ν‰κ°€)

### λ©ν‘ μ„±λ¥ (Gemma3:12b)
- **μ‘λ‹µ μ‹κ°„**: ν‰κ·  5-10μ΄ (ν΄λΌμ°λ“ ν™κ²½)
- **ν† ν° μƒμ„± μ†λ„**: μ΄λ‹Ή 20-30 ν† ν°
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: 24-32GB
- **ν•κΈ€ μ •ν™•λ„**: 80-90% (λ©ν‘)

## π”„ κ°λ° λ΅λ“λ§µ

### Phase 1: μ™„λ£ β…
- [x] μ„Έλ²• λ°μ΄ν„° μμ§‘ λ° μ •μ 
- [x] Phi-2 νμΈνλ‹ νμ΄ν”„λΌμΈ κµ¬μ¶•
- [x] κΈ°λ³Έ μ¶”λ΅  ν™κ²½ μ„¤μ •

### Phase 2: μ§„ν–‰ μ¤‘ π”„
- [x] ν„μ¬ λ¨λΈ ν•κ³„μ  λ¶„μ„
- [ ] Gemma3:12b νμΈνλ‹ ν™κ²½ μ¤€λΉ„
- [ ] ν–¥μƒλ λ°μ΄ν„°μ…‹ μƒμ„±

### Phase 3: κ³„ν μ¤‘ π“‹
- [ ] Gemma3:12b νμΈνλ‹ μ‹¤ν–‰
- [ ] μ„±λ¥ λΉ„κµ λ¶„μ„
- [ ] ν΄λΌμ°λ“ μ¶”λ΅  ν™κ²½ κµ¬μ¶•
- [ ] μ›Ή μΈν„°νμ΄μ¤ κ°λ°


**ν„μ¬ μƒνƒ**: Phi-2 λ¨λΈμ ν•κΈ€ μ²λ¦¬ ν•κ³„λ΅ μΈν•΄ Gemma3:12b λ¨λΈλ΅μ μ—…κ·Έλ μ΄λ“λ¥Ό κ³„ν μ¤‘μ…λ‹λ‹¤.
